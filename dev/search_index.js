var documenterSearchIndex = {"docs":
[{"location":"types/#Data-Types-and-Structures","page":"Data Types & Structures","title":"Data Types & Structures","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The following diagram showcases the type hierarchy of all BOSS inputs and hyperparameters.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"    \n  (Image: BOSS Pipeline)  \n    ","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"This reminder of this page contains documentation for all exported types and structures.","category":"page"},{"location":"types/#Problem-Definition","page":"Data Types & Structures","title":"Problem Definition","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The BossProblem structure contains the whole optimization problem definition together with the surrogate model.","category":"page"},{"location":"types/#BOSS.BossProblem","page":"Data Types & Structures","title":"BOSS.BossProblem","text":"BossProblem(; kwargs...)\n\nDefines the whole optimization problem for the BOSS algorithm.\n\nProblem Definition\n\nThere is some (noisy) blackbox function `y = f(x) = f_true(x) + ϵ` where `ϵ ~ Normal`.\n\nWe have some surrogate model `y = model(x) ≈ f_true(x)`\ndescribing our knowledge (or lack of it) about the blackbox function.\n\nWe wish to find `x ∈ domain` such that `fitness(f(x))` is maximized\nwhile satisfying the constraints `f(x) <= y_max`.\n\nKeywords\n\nfitness::Fitness: The Fitness function mapping the output to a real valued score.\nf::Union{Function, Missing}: The objective blackbox function.\ndomain::Domain: The Domain of the input x.\ny_max::AbstractVector{<:Real}: The constraints on the output y.\nmodel::SurrogateModel: The SurrogateModel.\nparams::Union{FittedParams, Nothing}: The model parameters. Defaults to nothing.\ndata::ExperimentData: The data obtained by evaluating the objective function.\nconsistent::Bool: True iff the model_params have been fitted using the current data.       Is set to consistent = false after updating the dataset,       and to consistent = true after re-fitting the parameters.       Defaults to constistent = false.\n\nSee Also\n\nbo!\n\n\n\n\n\n","category":"type"},{"location":"types/#Fitness","page":"Data Types & Structures","title":"Fitness","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The Fitness type is used to define the fitness function textfit(y) rightarrow mathbbR and is passed to the BossProblem.","category":"page"},{"location":"types/#BOSS.Fitness","page":"Data Types & Structures","title":"BOSS.Fitness","text":"Fitness\n\nAn abstract type for a fitness function measuring the quality of an output y of the objective function.\n\nFitness is used by the AcquisitionFunction to determine promising points for future evaluations.\n\nAll fitness functions should implement:\n\n(::CustomFitness)(y::AbstractVector{<:Real}) -> fitness::Real\n\nAn exception is the NoFitness, which can be used for problem without a well defined fitness. In such case, an AcquisitionFunction which does not depend on Fitness must be used.\n\nSee also: NoFitness, LinFitness, NonlinFitness, AcquisitionFunction\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The NoFitness can be used in problems without defined fitness (such as active learning problems). It is the default option used if no fitness is provided to BossProblem. The NoFitness can only be used with a custom AcquisitionFunction that does not require fitness.","category":"page"},{"location":"types/#BOSS.NoFitness","page":"Data Types & Structures","title":"BOSS.NoFitness","text":"NoFitness()\n\nPlaceholder for problems with no defined fitness.\n\nBossProblem defined with NoFitness can only be solved with AcquisitionFunction not dependent on Fitness.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The LinFitness can be used to define a simple fitness function depending linearly on the objective function outputs.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"textfit(y) = alpha^T y","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Using LinFitness instead of NonlinFitness may allow for simpler/faster computation of some acquisition functions (if they are defined that way).","category":"page"},{"location":"types/#BOSS.LinFitness","page":"Data Types & Structures","title":"BOSS.LinFitness","text":"LinFitness(coefs::AbstractVector{<:Real})\n\nUsed to define a linear fitness function  measuring the quality of an output y of the objective function.\n\nMay provide better performance than the more general NonlinFitness as some acquisition functions can be calculated analytically with linear fitness functions whereas this may not be possible with a nonlinear fitness function.\n\nSee also: NonlinFitness\n\nExample\n\nA fitness function f(y) = y[1] + a * y[2] + b * y[3] can be defined as:\n\njulia> LinFitness([1., a, b])\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The NonlinFitness can be used to define an arbitrary fitness function.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"textfit(y) rightarrow mathbbR","category":"page"},{"location":"types/#BOSS.NonlinFitness","page":"Data Types & Structures","title":"BOSS.NonlinFitness","text":"NonlinFitness(fitness::Function)\n\nUsed to define a general nonlinear fitness function measuring the quality of an output y of the objective function.\n\nIf your fitness function is linear, use LinFitness which may provide better performance.\n\nSee also: LinFitness\n\nExample\n\njulia> NonlinFitness(y -> cos(y[1]) + sin(y[2]))\n\n\n\n\n\n","category":"type"},{"location":"types/#Input-Domain","page":"Data Types & Structures","title":"Input Domain","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The Domain structure is used to define the input domain x in textDomain. The domain is formalized as","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"beginaligned\n lb  x  ub \n d_i implies (x_i in mathbbZ) \n textcons(x)  0 \nendaligned","category":"page"},{"location":"types/#BOSS.Domain","page":"Data Types & Structures","title":"BOSS.Domain","text":"Domain(; kwargs...)\n\nDescribes the optimization domain.\n\nKeywords\n\nbounds::AbstractBounds: The basic box-constraints on x. This field is mandatory.\ndiscrete::AbstractVector{Bool}: Can be used to designate some dimensions       of the domain as discrete.\ncons::Union{Nothing, Function}: Used to define arbitrary nonlinear constraints on x.       Feasible points x must satisfy all(cons(x) .> 0.). An appropriate acquisition       maximizer which can handle nonlinear constraints must be used if cons is provided.       (See AcquisitionMaximizer.)\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.AbstractBounds","page":"Data Types & Structures","title":"BOSS.AbstractBounds","text":"const AbstractBounds = Tuple{<:AbstractVector{<:Real}, <:AbstractVector{<:Real}}\n\nDefines box constraints.\n\nExample: ([0, 0], [1, 1]) isa AbstractBounds\n\n\n\n\n\n","category":"type"},{"location":"types/#Output-Constraints","page":"Data Types & Structures","title":"Output Constraints","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Constraints on the output y can be defined using the y_max field of the BossProblem. Providing y_max to BossProblem defines the linear constraints y < y_max.","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Arbitrary nonlinear constraints can be defined by augmenting the objective function. For example to define the constraint y[1] * y[2] < c, one can define an augmented objective function","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"function f_c(x)\n    y = f(x)  # the original objective function\n    y_c = [y..., y[1] * y[2]]\n    return y_c\nend","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"and use","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"y_max = [fill(Inf, y_dim)..., c]","category":"page"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"where y_dim is the output dimension of the original objective function. Note that defining nonlinear constraints this way increases the output dimension of the objective function and thus the model definition has to be modified accordingly.","category":"page"},{"location":"types/#Surrogate-Model","page":"Data Types & Structures","title":"Surrogate Model","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The surrogate model is defined using the SurrogateModel type and passed to the BossProblem.","category":"page"},{"location":"types/#BOSS.SurrogateModel","page":"Data Types & Structures","title":"BOSS.SurrogateModel","text":"SurrogateModel\n\nAn abstract type for a surrogate model approximating the objective function.\n\nDefining Custom Surrogate Model\n\nTo define a custom surrogate model, define a new subtype of SurrogateModel and a new subtype of ModelParams:\n\nstruct CustomModel <: SurrogateModel ... end\nstruct CustomModelParams <: ModelParams{CustomModel} ... end\n\nThe following methods should be implemented for the new CustomModel and CustomModelParams types. The input parameters in square brackets (e.g. [::ExperimentData]) are optional. It is preferrable to omit them if it is possible for your model. See the docs of the individual functions for more information.\n\nAll models should implement at least one of:\n\nmodel_posterior(model::CustomModel, params::CustomModelParams, data::ExperimentData) -> (x -> mean, std)\nmodel_posterior_slice(model::CustomModel, params::CustomModelParams, data::ExperimentData, slice::Int) -> (x -> mean, std)\n\nAll models should implement:\n\ndata_loglike(::SurrogateModel, ::ExperimentData) -> (::ModelParams -> ::Real)\nparams_loglike(::SurrogateModel, [::ExperimentData]) -> (::ModelParams -> ::Real)\nsample_params(rng::AbstractRNG, model::CustomModel, [::ExperimentData]) -> ::CustomModelParams\nvectorize(::CustomModelParams) -> ::AbstractVector{<:Real}\ndevectorize(::CustomModelParams, ::AbstractVector{<:Real}) -> ::CustomModelParams\nbijector(::ModelParams) -> ::Bijector\nparam_priors(model::CustomModel) -> ::AbstractVector{<:Union{Distribution, NoPrior}}       (Or param_priors(model::CustomModel, data::ExperimentData) -> ::AbstractVector{<:Union{Distribution, NoPrior}}       if the parameter priors depends on the data.)\n\nModels may implement:\n\nmake_discrete(model::CustomModel, discrete::AbstractVector{Bool}) -> discrete_model::CustomModel\nsliceable(::CustomModel) = true (false by default)\n\nIf sliceable(::CustomModel) == true, then the model should additionally implement:\n\nslice(model::CustomModel, slice::Int) -> model_slice::CustomModel\nslice(params::CustomModelParams, slice::Int) -> slice::CustomModelParams\njoin_slices(slices::AbstractVector{CustomModelParams}) -> params::CustomModelParams\n\nSee Also\n\nLinearModel, NonlinearModel, GaussianProcess, Semiparametric\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Each subtype of SurrogateModel has its own subtype of ModelParams defined, which stores all its (hyper)parameters.","category":"page"},{"location":"types/#BOSS.ModelParams","page":"Data Types & Structures","title":"BOSS.ModelParams","text":"ModelParams{M<:SurrogateModel}\n\nContains all parameters of the SurrogateModel M.\n\nSee SurrogateModel for more information.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The LinearModel and NonlinearModel structures are used to define parametric models. (Some compuatations are simpler/faster with linear model, so the LinearModel might provide better performance in the future. This functionality is not implemented yet, so using the NonlinearModel is equiavalent for now.)","category":"page"},{"location":"types/#BOSS.Parametric","page":"Data Types & Structures","title":"BOSS.Parametric","text":"Parametric{N}\n\nAn abstract type for parametric surrogate models.\n\nThe model function can be reconstructed using the following functions:\n\n(::Parametric)() -> ((x, θ) -> y)\n(::Parametric)(θ::AbstractVector{<:Real}) -> (x -> y)\n(::Parametric)(x::AbstractVector{<:Real}, θ::AbstractVector{<:Real}) -> y\n\nThe parametric type N <: Union{Nothing, NoiseStdPriors} determines whether the model is deterministic or probabilistic.\n\nA deterministic version of a Parametric model has N = nothing, does not implement the SurrogateModel API and cannot be used as a standalone model. It is mainly used as a part of the Semiparametric model.\n\nA probabilistic version of a Parametric model has defined noise_std_priors, implements the whole SurrogateModel API, and can be used as a standalone model.\n\nSee also: LinearModel, NonlinearModel\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.LinearModel","page":"Data Types & Structures","title":"BOSS.LinearModel","text":"LinearModel(; kwargs...)\n\nA parametric surrogate model linear in its parameters.\n\nThis model definition will provide better performance than the more general 'NonlinearModel' in the future. This feature is not implemented yet so it is equivalent to using NonlinearModel for now.\n\nThe linear model is defined as\n\n     ϕs = lift(x)\n     y = [θs[i]' * ϕs[i] for i in 1:m]\n\nwhere\n\n     x = [x₁, ..., xₙ]\n     y = [y₁, ..., yₘ]\n     θs = [θ₁, ..., θₘ], θᵢ = [θᵢ₁, ..., θᵢₚ]\n     ϕs = [ϕ₁, ..., ϕₘ], ϕᵢ = [ϕᵢ₁, ..., ϕᵢₚ]\n\nand n m p  R.\n\nKeywords\n\nlift::Function: Defines the lift function (::Vector{<:Real}) -> (::Vector{Vector{<:Real}})       according to the definition above.\ntheta_priors::ThetaPriors: The prior distributions for       the parameters [θ₁₁, ..., θ₁ₚ, ..., θₘ₁, ..., θₘₚ] according to the definition above.\nsoftplus_params::Union{Nothing, AbstractVector{Bool}}: A binary vector of the same length       as θ indicating which parameters are strictly positive and should be mapped via the softplus       function during optimization. Defaults to nothing meaning all parameters are real-valued.\ndiscrete::Union{Nothing, AbstractVector{Bool}}: A vector of booleans indicating       which dimensions of x are discrete. If discrete = nothing, all dimensions are continuous.       Defaults to nothing.\nnoise_std_priors::Union{Nothing, NoiseStdPriors}:       The prior distributions of the noise standard deviations of each y dimension.       If the model is used by itself, the noise_std_priors must be defined.       If the model is used as a part of the Semiparametric model, the noise_std_priors       must be left undefined, as the evaluation noise is modeled by the GaussianProcess in that case.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.NonlinearModel","page":"Data Types & Structures","title":"BOSS.NonlinearModel","text":"NonlinearModel(; kwargs...)\n\nA parametric surrogate model.\n\nIf your model is linear, you can use LinearModel which will provide better performance in the future. (Not yet implemented.)\n\nDefine the model as y = predict(x, θ) where θ are the model parameters.\n\nKeywords\n\npredict::Function: The predict function according to the definition above.\ntheta_priors::ThetaPriors: The prior distributions for the model parameters.\nsoftplus_params::Union{Nothing, AbstractVector{Bool}}: A binary vector of the same length       as θ indicating which parameters are strictly positive and should be mapped via the softplus       function during optimization. Defaults to nothing meaning all parameters are real-valued.\ndiscrete::Union{Nothing, AbstractVector{Bool}}: A vector of booleans indicating       which dimensions of x are discrete. If discrete = nothing, all dimensions are continuous.       Defaults to nothing.\nnoise_std_priors::Union{Nothing, NoiseStdPriors}:       The prior distributions of the noise standard deviations of each y dimension.       If the model is used by itself, the noise_std_priors must be defined.       If the model is used as a part of the Semiparametric model, the noise_std_priors       must be left undefined, as the evaluation noise is modeled by the GaussianProcess in that case.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.ParametricParams","page":"Data Types & Structures","title":"BOSS.ParametricParams","text":"ParametricParams(θ, σ, softplus_params)\n\nThe parameters of the Parametric model.\n\nParameters\n\nθ::AbstractVector{<:Real}: The parameters of the Parametric model.\nσ::AbstractVector{<:Real}: The noise standard deviations.\n\nMiscellaneous\n\nsoftplus_params::Union{Nothing, AbstractVector{Bool}}: The softplus_params       field copied from the Parametric model.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The GaussianProcess structure is used to define a Gaussian process model. See [1] for more information about Gaussian processes.","category":"page"},{"location":"types/#BOSS.Nonparametric","page":"Data Types & Structures","title":"BOSS.Nonparametric","text":"Nonparametric\n\nAn alias for GaussianProcess.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.GaussianProcess","page":"Data Types & Structures","title":"BOSS.GaussianProcess","text":"GaussianProcess(; kwargs...)\n\nA Gaussian Process surrogate model. Each output dimension is modeled by a separate independent process.\n\nKeywords\n\nmean::Union{Nothing, Function}: Used as the mean function for the GP.       Defaults to nothing equivalent to x -> zeros(y_dim).\nkernel::Kernel: The kernel used in the GP. Defaults to the Matern32Kernel().\nlengthscale_priors::LengthscalePriors: The prior distributions       for the length scales of the GP. The lengthscale_priors should be a vector       of y_dim x_dim-variate distributions where x_dim and y_dim are       the dimensions of the input and output of the model respectively.\namplitude_priors::AmplitudePriors: The prior distributions       for the amplitude hyperparameters of the GP. The amplitude_priors should be a vector       of y_dim univariate distributions.\nnoise_std_priors::NoiseStdPriors: The prior distributions       of the noise standard deviations of each y dimension.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.GaussianProcessParams","page":"Data Types & Structures","title":"BOSS.GaussianProcessParams","text":"GaussianProcessParams(λ, α, σ)\n\nThe parameters of the GaussianProcess model.\n\nParameters\n\nλ::AbstractMatrix{<:Real}: The length scales of the GP.\nα::AbstractVector{<:Real}: The amplitudes of the GP.\nσ::AbstractVector{<:Real}: The noise standard deviations.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The Semiparametric structure is used to define a semiparametric model combining the parametric and nonparametric (Gaussian process) models.","category":"page"},{"location":"types/#BOSS.Semiparametric","page":"Data Types & Structures","title":"BOSS.Semiparametric","text":"Semiparametric(; kwargs...)\n\nA semiparametric surrogate model (a combination of a Parametric model and a GaussianProcess).\n\nThe parametric model is used as the mean of the Gaussian Process and the evaluation noise is modeled by the Gaussian Process. All parameters of the models are estimated simultaneously.\n\nKeywords\n\nparametric::Parametric: The parametric model used as the GP mean function.\nnonparametric::Nonparametric{Nothing}: The outer GP model without mean.\n\nNote that the parametric model should be defined without noise priors, and the nonparametric model should be defined without mean function.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.SemiparametricParams","page":"Data Types & Structures","title":"BOSS.SemiparametricParams","text":"SemiparametricParams(θ, λ, α, σ, softplus_params)\n\nThe parameters of the [Semiparametric]@ref model.\n\nParameters\n\nθ::AbstractVector{<:Real}: The parameters of the parametric model.\nλ::AbstractMatrix{<:Real}: The length scales of the GP.\nα::AbstractVector{<:Real}: The amplitudes of the GP.\nσ::AbstractVector{<:Real}: The noise standard deviations.\n\nMiscellaneous\n\nsoftplus_params::Union{Nothing, AbstractVector{Bool}}: The softplus_params       field copied from the Parametric model.\n\n\n\n\n\n","category":"type"},{"location":"types/#Model-Parameters","page":"Data Types & Structures","title":"Model Parameters","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The fitted model parameters are stored in subtypes of FittedParams in the params field of the BossProblem.","category":"page"},{"location":"types/#BOSS.FittedParams","page":"Data Types & Structures","title":"BOSS.FittedParams","text":"FittedParams{M<:SurrogateModel}\n\nThe subtypes of FittedParams contain ModelParams fitted to the data via different methods.\n\nThere are two abstract subtypes of FittedParams:\n\nUniFittedParams: Contains a single ModelParams instance fitted to the data.\nMultiFittedParams: Contains multiple ModelParams samples sampled according to the data.\n\nThe contained ModelParams can be obtained via the get_params(::FittedParams) function, which return either a single ModelParams object or a vector of ModelParams objects.\n\nAll subtypes of UniFittedParams implement:\n\nget_params(::FittedParams) -> ::ModelParams\nslice(::FittedParams, idx::Int) -> ::FittedParams\n\nAll subtypes of MultiFittedParams implement:\n\nget_params(::FittedParams) -> ::Vector{<:ModelParams}\nslice(::FittedParams, idx::Int) -> ::FittedParams\n\nSee Also\n\nMAPParams, BIParams, RandomParams.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"Different model fitters create different subtypes of FittedParams. For example, MAP model fitters result in the MAPParams containing the MAP ModelParams, and variational model fitters results in the BIParams containing the posterior ModelParams samples.","category":"page"},{"location":"types/#BOSS.MAPParams","page":"Data Types & Structures","title":"BOSS.MAPParams","text":"MAPParams{M<:SurrogateModel}\n\nModelParams estimated via MAP estimation.\n\nKeywords\n\nparams::ModelParams{M}: The fitted model parameters.\nloglike::Float64: The log likelihood of the fitted parameters.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.BIParams","page":"Data Types & Structures","title":"BOSS.BIParams","text":"BIParams{M<:SurrogateModel, P<:ModelParams{M}}\n\nContains ModelParams samples obtained via (approximate) Bayesian inference.\n\nThe individual ModelParams samples can be obtained by iterating over the BIParams object.\n\nKeywords\n\nsamples::Vector{P}: A vector of the individual model parameter samples.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.RandomParams","page":"Data Types & Structures","title":"BOSS.RandomParams","text":"RandomParams{M<:SurrogateModel}\n\nA single random ModelParams sample from the prior.\n\nKeywords\n\nparams::ModelParams{M}: The random model parameters.\n\n\n\n\n\n","category":"type"},{"location":"types/#Experiment-Data","page":"Data Types & Structures","title":"Experiment Data","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The data from all past objective function evaluations are stored in the ExperimentData structure. It is also used to provide the intial training data to BossProblem.","category":"page"},{"location":"types/#BOSS.ExperimentData","page":"Data Types & Structures","title":"BOSS.ExperimentData","text":"ExperimentData(X, Y)\n\nStores all the data collected during the optimization.\n\nAt least one initial datapoint has to be provided (purely for implementation reasons). One can for example use LatinHypercubeSampling.jl to obtain a small intial grid, or provide a single random initial datapoint.\n\nKeywords\n\nX::AbstractMatrix{<:Real}: Contains the objective function inputs as columns.\nY::AbstractMatrix{<:Real}: Contains the objective function outputs as columns.\n\n\n\n\n\n","category":"type"},{"location":"types/#Model-Fitter","page":"Data Types & Structures","title":"Model Fitter","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The ModelFitter type defines the algorithm used to estimate the model (hyper)parameters.","category":"page"},{"location":"types/#BOSS.ModelFitter","page":"Data Types & Structures","title":"BOSS.ModelFitter","text":"ModelFitter{T<:FittedParams}\n\nSpecifies the library/algorithm used for model parameter estimation. The parametric type T specifies the subtype of FittedParams returned by the model fitter.\n\nDefining Custom Model Fitter\n\nDefine a custom model fitter algorithm by defining a new subtype of ModelFitter.\n\nExample: struct CustomFitter <: ModelFitter{MAPParams} ... end\n\nAll model fitters should implement: `estimateparameters(modelfitter::CustomFitter, problem::BossProblem, options::BossOptions; return_all::Bool) -> ::FittedParams\n\nSee Also\n\nOptimizationMAP, TuringBI\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The OptimizationMAP model fitter can be used to utilize any optimization algorithm from the Optimization.jl package in order to find the MAP estimate of the (hyper)parameters. (See the example usage.)","category":"page"},{"location":"types/#BOSS.OptimizationMAP","page":"Data Types & Structures","title":"BOSS.OptimizationMAP","text":"OptimizationMAP(; kwargs...)\n\nFinds the MAP estimate of the model parameters and hyperparameters using the Optimization.jl package.\n\nTo use this model fitter, first add one of the Optimization.jl packages (e.g. OptimizationPRIMA) to load some optimization algorithms which are passed to the OptimizationMAP constructor.\n\nKeywords\n\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Union{Int, AbstractVector{<:ModelParams}}: The number of optimization restarts,       or a vector of ModelParams containing initial (hyper)parameter values for the optimization runs.\nparallel::Bool: If parallel=true then the individual restarts are run in parallel.\nautodiff::Union{SciMLBase.AbstractADType, Nothing}:: The automatic differentiation module   passed to Optimization.OptimizationFunction. \nkwargs::Base.Pairs{Symbol, <:Any}: Other kwargs are passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The TuringBI model fitter can be used to utilize the Turing.jl library in order to sample the (hyper)parameters from the posterior given by the current dataset.","category":"page"},{"location":"types/#BOSS.TuringBI","page":"Data Types & Structures","title":"BOSS.TuringBI","text":"TuringBI(; kwargs...)\n\nSamples the model parameters and hyperparameters using the Turing.jl package.\n\nTo use this model fitter, first add the Turing.jl package.\n\nKeywords\n\nsampler::Any: The sampling algorithm used to draw the samples.\nwarmup::Int: The amount of initial unused 'warmup' samples in each chain.\nsamples_in_chain::Int: The amount of samples used from each chain.\nchain_count::Int: The amount of independent chains sampled.\nleap_size: Every leap_size-th sample is used from each chain. (To avoid correlated samples.)\nparallel: If parallel=true then the chains are sampled in parallel.\n\nSampling Process\n\nIn each sampled chain;\n\nThe first warmup samples are discarded.\nFrom the following leap_size * samples_in_chain samples each leap_size-th is kept.\n\nThen the samples from all chains are concatenated and returned.\n\nTotal drawn samples:    'chaincount * (warmup + leapsize * samplesinchain)' Total returned samples: 'chaincount * samplesin_chain'\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SamplingMAP model fitter preforms MAP estimation by sampling the parameters from their priors and maximizing the posterior probability over the samples. This is a trivial model fitter suitable for simple experimentation with BOSS.jl and/or Bayesian optimization. A more sophisticated model fitter such as OptimizationMAP or TuringBI should be used to solve real problems.","category":"page"},{"location":"types/#BOSS.SamplingMAP","page":"Data Types & Structures","title":"BOSS.SamplingMAP","text":"SamplingMAP()\n\nOptimizes the model parameters by sampling them from their prior distributions and selecting the best sample in sense of MAP.\n\nKeywords\n\nsamples::Int: The number of drawn samples.\nparallel::Bool: The sampling is performed in parallel if parallel=true.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The RandomFitter model fitter samples random parameter values from their priors. It does NOT optimize for the most probable parameters in any way. This model fitter is provided solely for easy experimentation with BOSS.jl and should not be used to solve problems.","category":"page"},{"location":"types/#BOSS.RandomFitter","page":"Data Types & Structures","title":"BOSS.RandomFitter","text":"RandomFitter()\n\nReturns random model parameters sampled from their respective priors.\n\nCan be useful with RandomSelectAM to avoid unnecessary model parameter estimations.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SampleOptMAP model fitter combines the SamplingMAP and OptimizationMAP. It first samples many model parameter samples from their priors, and subsequently runs multiple optimization runs initiated at the best samples.","category":"page"},{"location":"types/#BOSS.SampleOptMAP","page":"Data Types & Structures","title":"BOSS.SampleOptMAP","text":"SampleOptMAP(; kwargs...)\nSampleOptMAP(::SamplingMAP, ::OptimizationMAP)\n\nCombines SamplingMAP and OptimizationMAP to first sample many parameter samples from the prior, and subsequently start multiple optimization runs initialized from the best samples.\n\nKeywords\n\nsamples::Int: The number of drawn samples.\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Int: The number of optimization restarts.\nparallel::Bool: If parallel=true, then both the sampling and the optimization are performed in parallel.\nsoftplus_hyperparams::Bool: If softplus_hyperparams=true then the softplus function       is applied to GP hyperparameters (length-scales & amplitudes) and noise deviations       to ensure positive values during optimization.\nsoftplus_params::Union{Bool, Vector{Bool}}: Defines to which parameters of the parametric       model should the softplus function be applied to ensure positive values.       Supplying a boolean instead of a binary vector turns the softplus on/off for all parameters.       Defaults to false meaning the softplus is applied to no parameters.\n\n\n\n\n\n","category":"type"},{"location":"types/#Acquisition-Maximizer","page":"Data Types & Structures","title":"Acquisition Maximizer","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The AcquisitionMaximizer type is used to define the algorithm used to maximize the acquisition function.","category":"page"},{"location":"types/#BOSS.AcquisitionMaximizer","page":"Data Types & Structures","title":"BOSS.AcquisitionMaximizer","text":"AcquisitionMaximizer\n\nSpecifies the library/algorithm used for acquisition function optimization.\n\nDefining Custom Acquisition Maximizer\n\nTo define a custom acquisition maximizer, define a new subtype of AcquisitionMaximizer.\n\nstruct CustomAlg <: AcquisitionMaximizer ... end\n\nAll acquisition maximizers should implement: maximize_acquisition(acq_maximizer::CustomAlg, acq::AcquisitionFunction, problem::BossProblem, options::BossOptions) -> (x, val).\n\nThis method should return a tuple (x, val). The returned vector x is the point of the input domain which maximizes the given acquisition function acq (as a vector), or a batch of points (as a column-wise matrix). The returned val is the acquisition value acq(x), or the values acq.(eachcol(x)) for each point of the batch, or nothing (depending on the acquisition maximizer implementation).\n\nSee also: OptimizationAM\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The OptimizationAM can be used to utilize any optimization algorithm from the Optimization.jl package.","category":"page"},{"location":"types/#BOSS.OptimizationAM","page":"Data Types & Structures","title":"BOSS.OptimizationAM","text":"OptimizationAM(; kwargs...)\n\nMaximizes the acquisition function using the Optimization.jl library.\n\nCan handle constraints on x if according optimization algorithm is selected.\n\nKeywords\n\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Union{Int, AbstractMatrix{<:Real}}: The number of optimization restarts,       or a matrix of optimization intial points as columns.\nparallel::Bool: If parallel=true then the individual restarts are run in parallel.\nautodiff:SciMLBase.AbstractADType:: The automatic differentiation module       passed to Optimization.OptimizationFunction.\nkwargs...: Other kwargs are passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The GridAM maximizes the acquisition function by evaluating all points on a fixed grid of points. This is a trivial acquisition maximizer suitable for simple experimentation with BOSS.jl and/or Bayesian optimization. More sophisticated acquisition maximizers such as OptimizationAM should be used to solve real problems.","category":"page"},{"location":"types/#BOSS.GridAM","page":"Data Types & Structures","title":"BOSS.GridAM","text":"GridAM(kwargs...)\n\nMaximizes the acquisition function by checking a fine grid of points from the domain.\n\nExtremely simple optimizer which can be used for simple problems or for debugging. Not suitable for problems with high dimensional domain.\n\nCan be used with constraints on x.\n\nKeywords\n\nproblem::BossProblem: Provide your defined optimization problem.\nsteps::Vector{Float64}: Defines the size of the grid gaps in each x dimension.\nparallel::Bool: If parallel=true, the optimization is parallelized. Defaults to true.\nshuffle::Bool: If shuffle=true, the grid points are shuffled before each optimization. Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SamplingAM samples random candidate points from the given x_prior distribution and selects the sample with maximal acquisition value.","category":"page"},{"location":"types/#BOSS.SamplingAM","page":"Data Types & Structures","title":"BOSS.SamplingAM","text":"SamplingAM(; kwargs...)\n\nOptimizes the acquisition function by sampling candidates from the user-provided prior, and returning the sample with the highest acquisition value.\n\nKeywords\n\nx_prior::MultivariateDistribution: The prior over the input domain used to sample candidates.\nsamples::Int: The number of samples to be drawn and evaluated.\nparallel::Bool: If parallel=true then the sampling is parallelized. Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The RandomAM simply returns a random point. It does NOT perform any optimization. This acquisition maximizer is provided solely for easy experimentation with BOSS.jl and should not be used to solve problems.","category":"page"},{"location":"types/#BOSS.RandomAM","page":"Data Types & Structures","title":"BOSS.RandomAM","text":"RandomAM()\n\nSelects a random interior point instead of maximizing the acquisition function. Can be used for method comparison.\n\nCan handle constraints on x, but does so by generating random points in the box domain until a point satisfying the constraints is found. Therefore it can take a long time or even get stuck if the constraints are very tight.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The GivenPointAM always return the same evaluation point predefined by the user. The GivenSequenceAM returns the predefined sequence of evaluation points and throws an error once it runs out of points. These dummy acquisition maximizers are useful for controlled experiments.","category":"page"},{"location":"types/#BOSS.GivenPointAM","page":"Data Types & Structures","title":"BOSS.GivenPointAM","text":"GivenPointAM(x::Vector{...})\n\nA dummy acquisition maximizer that always returns predefined point x.\n\nSee Also\n\nGivenSequenceAM,\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.GivenSequenceAM","page":"Data Types & Structures","title":"BOSS.GivenSequenceAM","text":"GivenSequenceAM(X::Matrix{...})\nGivenSequenceAM(X::Vector{Vector{...}})\n\nA dummy acquisition maximizer that returns the predefined sequence of points in the given order. The maximizer throws an error if it runs out of points in the sequence.\n\nSee Also\n\nGivenPointAM,\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SampleOptAM samples many candidate points from the given x_prior distribution, and subsequently performs multiple optimization runs initiated from the best samples.","category":"page"},{"location":"types/#BOSS.SampleOptAM","page":"Data Types & Structures","title":"BOSS.SampleOptAM","text":"SampleOptAM(; kwargs...)\n\nOptimizes the acquisition function by first sampling candidates from the user-provided prior, and then running multiple optimization runs initiated from the samples with the highest acquisition values.\n\nKeywords\n\nx_prior::MultivariateDistribution: The prior over the input domain used to sample candidates.\nsamples::Int: The number of samples to be drawn and evaluated.\nalgorithm::Any: Defines the optimization algorithm.\nmultistart::Int: The number of optimization restarts.\nparallel::Bool: If parallel=true, both the sampling and individual optimization runs       are performed in parallel.\nautodiff:SciMLBase.AbstractADType:: The automatic differentiation module       passed to Optimization.OptimizationFunction.\nkwargs...: Other kwargs are passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The SequentialBatchAM can be used as a wrapper of any of the other acquisition maximizers. It returns a batch of promising points for future evaluations instead of a single point, and thus allows for evaluation of the objective function in batches.","category":"page"},{"location":"types/#BOSS.SequentialBatchAM","page":"Data Types & Structures","title":"BOSS.SequentialBatchAM","text":"SequentialBatchAM(::AcquisitionMaximizer, ::Int)\nSequentialBatchAM(; am, batch_size)\n\nProvides multiple candidates for batched objective function evaluation.\n\nSelects the candidates sequentially by iterating the following steps:\n\nUse the 'inner' acquisition maximizer to select a candidate x.\nExtend the dataset with a 'speculative' new data point\ncreated by taking the candidate x and the posterior predictive mean of the surrogate ŷ.\nIf batch_size candidates have been selected, return them.\nOtherwise, goto step 1).\n\nKeywords\n\nam::AcquisitionMaximizer: The inner acquisition maximizer.\nbatch_size::Int: The number of candidates to be selected.\n\n\n\n\n\n","category":"type"},{"location":"types/#Acquisition-Function","page":"Data Types & Structures","title":"Acquisition Function","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The acquisition function is defined using the AcquisitionFunction type.","category":"page"},{"location":"types/#BOSS.AcquisitionFunction","page":"Data Types & Structures","title":"BOSS.AcquisitionFunction","text":"AcquisitionFunction\n\nSpecifies the acquisition function describing the \"quality\" of a potential next evaluation point.\n\nDefining Custom Acquisition Function\n\nTo define a custom acquisition function, define a new subtype of AcquisitionFunction.\n\nstruct CustomAcq <: AcquisitionFunction ... end\n\nAll acquisition functions should implement: (acquisition::CustomAcq)(problem::BossProblem, options::BossOptions) -> (x -> ::Real)\n\nThis method should return a function acq(x::AbstractVector{<:Real}) = val::Real, which is maximized to select the next evaluation function of blackbox function in each iteration.\n\nSee Also\n\nExpectedImprovement\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The ExpectedImprovement defines the expected improvement acquisition function. See [1] for more information.","category":"page"},{"location":"types/#BOSS.ExpectedImprovement","page":"Data Types & Structures","title":"BOSS.ExpectedImprovement","text":"ExpectedImprovement(; kwargs...)\n\nThe expected improvement (EI) acquisition function.\n\nFitness function must be defined as a part of the problem definition in order to use EI. (See Fitness.)\n\nMeasures the quality of a potential evaluation point x as the expected improvement in best-so-far achieved fitness by evaluating the objective function at y = f(x).\n\nIn case of constrained problems, the expected improvement is additionally weighted by the probability of feasibility of y. I.e. the probability that all(cons(y) .> 0.).\n\nIf the problem is constrained on y and no feasible point has been observed yet, then the probability of feasibility alone is returned as the acquisition function.\n\nRather than using the actual evaluations (xᵢ,yᵢ) from the dataset, the best-so-far achieved fitness is calculated as the maximum fitness among the means ̂yᵢ of the posterior predictive distribution of the model evaluated at xᵢ. This is a simple way to handle evaluation noise which may not be suitable for problems with substantial noise. In case of Bayesian Inference, an averaged posterior of the model posterior samples is used for the prediction of ŷᵢ.\n\nKeywords\n\nϵ_samples::Int: Controls how many samples are used to approximate EI.       The ϵ_samples keyword is ignored unless MAP model fitter and NonlinFitness are used!       In case of BI model fitter, the number of samples is instead set equal to the number of posterior samples.       In case of LinearFitness, the expected improvement can be calculated analytically.\ncons_safe::Bool: If set to true, the acquisition function acq(x) is made 'constraint-safe'       by checking the bounds and constraints during each evaluation.       Set cons_safe to true if the evaluation of the model at exterior points       may cause errors or nonsensical values.       You may set cons_safe to false if the evaluation of the model at exterior points       can provide useful information to the acquisition maximizer and does not cause errors.       Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"types/#Termination-Conditions","page":"Data Types & Structures","title":"Termination Conditions","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The TermCond type is used to define the termination condition of the BO procedure.","category":"page"},{"location":"types/#BOSS.TermCond","page":"Data Types & Structures","title":"BOSS.TermCond","text":"TermCond\n\nSpecifies the termination condition of the whole BOSS algorithm. Inherit this type to define a custom termination condition.\n\nExample: struct CustomCond <: TermCond ... end\n\nAll termination conditions should implement: (cond::CustomCond)(problem::BossProblem)\n\nThis method should return true to keep the optimization running and return false once the optimization is to be terminated.\n\nSee also: IterLimit\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The NoLimit can be used to let the algorithm run indefinitely.","category":"page"},{"location":"types/#BOSS.NoLimit","page":"Data Types & Structures","title":"BOSS.NoLimit","text":"NoLimit()\n\nNever terminates.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The IterLimit terminates the procedure after a predefined number of iterations.","category":"page"},{"location":"types/#BOSS.IterLimit","page":"Data Types & Structures","title":"BOSS.IterLimit","text":"IterLimit(iter_max::Int)\n\nTerminates the BOSS algorithm after predefined number of iterations.\n\nSee also: bo!\n\n\n\n\n\n","category":"type"},{"location":"types/#Miscellaneous","page":"Data Types & Structures","title":"Miscellaneous","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The BossOptions structure is used to define miscellaneous hyperparameters of the BOSS.jl package.","category":"page"},{"location":"types/#BOSS.BossOptions","page":"Data Types & Structures","title":"BOSS.BossOptions","text":"BossOptions(; kwargs...)\n\nStores miscellaneous settings of the BOSS algorithm.\n\nKeywords\n\ninfo::Bool: Setting info=false silences the BOSS algorithm.\ndebug::Bool: Set debug=true to print stactraces of caught optimization errors.\nparallel_evals::Symbol: Possible values: :serial, :parallel, :distributed. Defaults to :parallel.       Determines whether to run multiple objective function evaluations       within one batch in serial, parallel, or distributed fashion.       (Only has an effect if batching AM is used.)\ncallback::BossCallback: If provided, callback(::BossProblem; kwargs...)       will be called before the BO procedure starts and after every iteration.\n\nSee also: bo!\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The BossCallback type is used to pass callbacks which will be called in every iteration of the BO procedure (and once before the procedure starts).","category":"page"},{"location":"types/#BOSS.BossCallback","page":"Data Types & Structures","title":"BOSS.BossCallback","text":"BossCallback\n\nIf a BossCallback is provided to BossOptions, the callback is called once before the BO procedure starts, and after each iteration.\n\nAll callbacks should implement:\n\n(::CustomCallback)(::BossProblem;       ::ModelFitter,       ::AcquisitionMaximizer,       ::AcquisitionFunction,       ::TermCond,       ::BossOptions,       first::Bool,   )\n\nThe kwarg first is true only on the first callback before the BO procedure starts.\n\nSee PlotCallback for an example usage of a callback for plotting.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSS.NoCallback","page":"Data Types & Structures","title":"BOSS.NoCallback","text":"NoCallback()\n\nDoes nothing.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"The PlotCallback provides plots the state of the BO procedure in every iteration. It currently only supports one-dimensional input spaces.","category":"page"},{"location":"types/#BOSS.PlotCallback","page":"Data Types & Structures","title":"BOSS.PlotCallback","text":"PlotOptions(Plots; kwargs...)\n\nIf PlotOptions is passed to BossOptions as callback, the state of the optimization problem is plotted in each iteration. Only works with one-dimensional x domains but supports multi-dimensional y.\n\nArguments\n\nPlots::Module: Evaluate using Plots and pass the Plots module to PlotsOptions.\n\nKeywords\n\nf_true::Union{Nothing, Function}: The true objective function to be plotted.\npoints::Int: The number of points in each plotted function.\nxaxis::Symbol: Used to change the x axis scale (:identity, :log).\nyaxis::Symbol: Used to change the y axis scale (:identity, :log).\ntitle::String: The plot title.\n\n\n\n\n\n","category":"type"},{"location":"types/#References","page":"Data Types & Structures","title":"References","text":"","category":"section"},{"location":"types/","page":"Data Types & Structures","title":"Data Types & Structures","text":"[1] Bobak Shahriari et al. “Taking the human out of the loop: A review of Bayesian optimization”. In: Proceedings of the IEEE 104.1 (2015), pp. 148–175","category":"page"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This page contains the documentation for all exported functions.","category":"page"},{"location":"functions/#Main-Function","page":"Functions","title":"Main Function","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The main function bo!(::BossProblem; kwargs...) performs the Bayesian optimization. It augments the dataset and updates the model parameters and/or hyperparameters stored in problem.data.","category":"page"},{"location":"functions/#BOSS.bo!","page":"Functions","title":"BOSS.bo!","text":"bo!(problem::BossProblem{Function}; kwargs...)\nx = bo!(problem::BossProblem{Missing}; kwargs...)\n\nRun the Bayesian optimization procedure to solve the given optimization problem or give a recommendation for the next evaluation point if problem.f == missing.\n\nArguments\n\nproblem::BossProblem: Defines the optimization problem.\n\nKeywords\n\nmodel_fitter::ModelFitter: Defines the algorithm used to estimate model parameters.\nacq_maximizer::AcquisitionMaximizer: Defines the algorithm used to maximize the acquisition function.\nacquisition::AcquisitionFunction: Defines the acquisition function maximized to select       promising candidates for further evaluation.\nterm_cond::TermCond: Defines the termination condition.\noptions::BossOptions: Defines miscellaneous settings.\n\nReferences\n\nBossProblem, ModelFitter, AcquisitionMaximizer, TermCond, BossOptions\n\nExamples\n\nSee 'https://soldasim.github.io/BOSS.jl/stable/example/' for example usage.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The following diagram showcases the pipeline of the main function. The package can be used in two modes;","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The \"BO mode\" is used if the objective function is defined within the BossProblem. In this mode, BOSS performs the standard Bayesian optimization procedure while querying the objective function for new points.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The \"Recommender mode\" is used if the objective function is missing. In this mode, BOSS performs a single iteration of the Bayesian optimization procedure and returns a recommendation for the next evaluation point. The user can evaluate the objective function manually, use the method augment_dataset! to add the result to the data, and call BOSS again for a new recommendation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"    \n  (Image: BOSS Pipeline)  \n    ","category":"page"},{"location":"functions/#Utility-Functions","page":"Functions","title":"Utility Functions","text":"","category":"section"},{"location":"functions/#BOSS.estimate_parameters!","page":"Functions","title":"BOSS.estimate_parameters!","text":"estimate_parameters!(::BossProblem, ::ModelFitter)\n\nEstimate the model parameters & hyperparameters using the given model_fitter algorithm.\n\nKeywords\n\noptions::BossOptions: Defines miscellaneous settings.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.maximize_acquisition","page":"Functions","title":"BOSS.maximize_acquisition","text":"x = maximize_acquisition(::BossProblem, ::AcquisitionFunction, ::AcquisitionMaximizer)\n\nMaximize the given acquisition function via the given acq_maximizer algorithm to find the optimal next evaluation point(s).\n\nKeywords\n\noptions::BossOptions: Defines miscellaneous settings.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.eval_objective!","page":"Functions","title":"BOSS.eval_objective!","text":"eval_objective!(::BossProblem, x::AbstractVector{<:Real})\n\nEvaluate the objective function and update the data.\n\nKeywords\n\noptions::BossOptions: Defines miscellaneous settings.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.update_parameters!","page":"Functions","title":"BOSS.update_parameters!","text":"update_parameters!(problem::BossProblem{F,M}, params::ModelParams{M})\n\nUpdate the model parameters.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.augment_dataset!","page":"Functions","title":"BOSS.augment_dataset!","text":"augment_dataset!(::BossProblem, x::AbstractVector{<:Real}, y::AbstractVector{<:Real})\naugment_dataset!(::BossProblem, X::AbstractMatrix{<:Real}, Y::AbstractMatrix{<:Real})\n\nAdd one (as vectors) or more (as matrices) datapoints to the dataset.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.model_posterior","page":"Functions","title":"BOSS.model_posterior","text":"model_posterior(::BossProblem) -> post(s)\nmodel_posterior(::SurrogateModel, ::ModelParams, ::ExperimentData) -> post(s)\nmodel_posterior(::SurrogateModel, ::FittedParams, ::ExperimentData) -> post(s)\n\nReturn the posterior predictive distribution of the surrogate model with two methods;\n\npost(x::AbstractVector{<:Real}) -> μs::AbstractVector{<:Real}, σs::AsbtractVector{<:Real}\npost(X::AbstractMatrix{<:Real}) -> μs::AbstractMatrix{<:Real}, Σs::AbstractArray{<:Real, 3}\n\nor a vector of such posterior functions (in case a ModelFitter which samples multiple ModelParams has been used).\n\nThe first method takes a single point x of length x_dim(::BossProblem) from the Domain, and returns the predictive means and deviations of the corresponding output vector y of length y_dim(::BossProblem) such that:\n\nμs, σs = post(x) => y ∼ product_distribution(Normal.(μs, σs))\nμs, σs = post(x) => y[i] ∼ Normal(μs[i], σs[i])\n\nThe second method takes multiple points from the Domain as a column-wise matrix X of size (x_dim, N), and returns the joint predictive means and covariance matrices of the corresponding output matrix Y of size (y_dim, N) such that:\n\nμs, Σs = post(X) => transpose(Y) ∼ product_distribution(MvNormal.(eachcol(μs), eachslice(Σs; dims=3)))\nμs, Σs = post(X) => Y[i,:] ∼ MvNormal(μs[:,i], Σs[:,:,i])\n\nSee also: model_posterior_slice\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.model_posterior_slice","page":"Functions","title":"BOSS.model_posterior_slice","text":"model_posterior_slice(::BossProblem, slice::Int) -> post\n\nReturn the posterior predictive distributions of the given output slice with two methods:\n\npost(x::AbstractVector{<:Real}) -> μ::Real, σ::Real\npost(X::AbstractMatrix{<:Real}) -> μ::AbstractVector{<:Real}, Σ::AbstractMatrix{<:Real}\n\nThe first method takes a single point x of length x_dim(::BossProblem) from the Domain, and returns the predictive mean and deviation of the corresponding output number y such that:\n\nμ, σ = post(x) => y ∼ Normal(μ, σ)\n\nThe second method takes multiple points from the Domain as a column-wise matrixXof size(x_dim, N), and returns the joint predictive mean and covariance matrix of the corresponding output vectoryof lengthN` such that:\n\nμ, Σ = post(X) => y ∼ MvNormal(μ, Σ)\n\nIn case one is only interested in predicting a certain output dimension, using model_posterior_slice can be more efficient than model_posterior (depending on the used SurrogateModel).\n\nNote that model_posterior_slice can be used even if sliceable(model) == false. It will, however, not provide any additional efficiency in such case.\n\nSee also: model_posterior\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.average_posterior","page":"Functions","title":"BOSS.average_posterior","text":"average_posterior(::AbstractVector{Function}) -> Function\n\nReturn an averaged posterior predictive distribution of the given posteriors.\n\nUseful with ModelFitters which sample multiple ModelParams samples.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.result","page":"Functions","title":"BOSS.result","text":"result(problem) -> (x, y)\n\nReturn the best found point (x, y).\n\nReturns the point (x, y) from the dataset of the given problem such that y satisfies the constraints and fitness(y) is maximized. Returns nothing if the dataset is empty or if no feasible point is present.\n\nDoes not check whether x belongs to the domain as no exterior points should be present in the dataset.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.x_dim","page":"Functions","title":"BOSS.x_dim","text":"x_dim(::BossProblem) -> Int\n\nReturn the input dimension of the problem.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.y_dim","page":"Functions","title":"BOSS.y_dim","text":"y_dim(::BossProblem) -> Int\n\nReturn the output dimension of the problem.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.cons_dim","page":"Functions","title":"BOSS.cons_dim","text":"cons_dim(::BossProblem) -> Int\ncons_dim(::Domain) -> Int\n\nReturn the output dimension of the constraint function on the input.\n\nSee Domain for more information.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.data_count","page":"Functions","title":"BOSS.data_count","text":"data_count(::BossProblem) -> Int\n\nReturn the number of datapoints in the dataset.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.is_consistent","page":"Functions","title":"BOSS.is_consistent","text":"is_consistent(::BossProblem) -> Bool\n\nReturn true iff the model parameters have been fitted using the current dataset.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSS.calc_inverse_gamma","page":"Functions","title":"BOSS.calc_inverse_gamma","text":"Return an Inverse Gamma distribution with approximately 0.99 probability mass between lb and ub.\n\n\n\n\n\n","category":"function"},{"location":"#BOSS.jl","page":"BOSS.jl","title":"BOSS.jl","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"BOSS stands for \"Bayesian Optimization with Semiparametric Surrogate\". BOSS.jl is a Julia package for Bayesian optimization. It provides a compact way to define an optimization problem and a surrogate model, and solve the problem. It allows changing the algorithms used for the subtasks of fitting the surrogate model and optimizing the acquisition function. Simple interfaces are defined for the use of custom surrogate models and/or algorithms for the subtasks.","category":"page"},{"location":"#Bayesian-Optimization","page":"BOSS.jl","title":"Bayesian Optimization","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"Bayesian optimization is an abstract algorithm for blackbox optimization (or active learning) with expensive-to-evaluate objective functions. The general procedure is showcased by the pseudocode below. [1]","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"    \n  (Image: Bayesian optimization)  \n    ","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The real-valued acquisition function alpha(x) defines the utility of a potential point for the next evaluation. Different acquisition functions are suitable for optimization and active learning problems. We maximize the acquisition function to select the most useful point for the next evaluation.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"Once the optimum x_n+1 of the acquisition function is obtained, we evaluate the blackbox objective function y_n+1 gets f(x_n+1) and we augment the dataset.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"Finally, we update the surrogate model according to the augmented dataset.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"See [1] for more information on Bayesian optimization.","category":"page"},{"location":"#Optimization-Problem","page":"BOSS.jl","title":"Optimization Problem","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The problem is defined using the BossProblem structure and it follows the formalization below.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"We have some (noisy) blackbox objective function","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"y = f(x) = f_t(x) + epsilon ","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"where epsilon sim mathcalN(0 sigma_f^2) is a Gaussian noise. We are able to evaluate f(x_i) and obtain a noisy realization","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"y_i sim mathcalN(f_t(x_i) sigma_f^2) ","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"Our goal is to solve the following optimization problem","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"beginaligned\ntextmax   textfit(y) \ntextst   y  y_textmax \n x in textDomain \nendaligned","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"where textfit(y) is a real-valued fitness function defined on the outputs, y_textmax is a vector defining constraints on outputs, and textDomain defines constraints on inputs.","category":"page"},{"location":"#Surrogate-Model","page":"BOSS.jl","title":"Surrogate Model","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The surrogate model approximates the objective function based on the available data. It is defined using the SurrogateModel type and passed to the BossProblem structure. The basic provided models are the Parametric model, the GaussianProcess, and the Semiparametric model combining the previous two.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The predictive distribution of the Parametric model","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"y sim mathcalN(m(x hattheta) hatsigma_f^2)","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"is given by the parametric function m(x theta), the parameter vector hattheta, and the estimated evaluation noise deviations hatsigma_f. The model is defined by the parametric function m(x theta) together with parameter priors theta_i sim p(theta_i). The parameters hattheta and the noise deviations hatsigma_f are estimated based on the current dataset.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The GaussianProcess (GP) is a nonparametric model, so its predictive distribution is based on the whole dataset instead of some vector of parameters. The predictive distribution is given by equations 29, 30 in [1]. The model is defined by defining priors over all its hyperparameters (length scales, amplitudes, and noise deviation).","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The Semiparametric model combines the previous two models. It is a Gaussian process, but uses the parametric model as the prior mean of the GP (the mu_0(x) function in equation 29 in [1]). An alternative way of interpreting the semiparametric model is that it fits the data using a parametric model and uses a Gaussian process to model the residual errors of the parametric model. The model is defined by defining both a Parametric and a GaussianProcess and passing them to the [Semiparametric] model.","category":"page"},{"location":"#The-Sub-Algorithms","page":"BOSS.jl","title":"The Sub-Algorithms","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The algorithms which are used to perform the sub-steps of the BO procedure are defined using the subtypes of the ModelFitter type and the AcquisitionMaximizer type. They are passed to the main function bo! as keyword arguments.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The model parameters can either be fitted in a MAP fashion (using for example the OptimizationMAP model fitter) or via Bayesian inference (using the TuringBI model fitter). Other model fitters are also available and custom ones can be defined by extending the ModelFitter type.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The most basic AcquisitionMaximizer is the OptimizationAM. Other acquisition maximizers are also available and custom ones can be defined by extending the AcquisitionMaximizer type. Batching of the objective function evaluations can be achieved using the SequentialBatchAM.","category":"page"},{"location":"#Acquisition-Function","page":"BOSS.jl","title":"Acquisition Function","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The acquisition function is defined using subtypes of the AcquisitionFunction type. The acquisition function is passed to the main function bo! as a keyword argument.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The most commonly used acquisition function, ExpectedImprovement, is provided. Custom acquisition functions can be defined by extending the AcquisitionFunction type.","category":"page"},{"location":"#Termination-Condition","page":"BOSS.jl","title":"Termination Condition","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The termination condition of the BO procedure can be defined using the TermCond type. The termination condition is passed to the main function bo! as a keyword argument.","category":"page"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The basic IterLimit termination condition is provided. Custom termination conditions can be defined by extending the TermCond type.","category":"page"},{"location":"#Active-Learning-Problem","page":"BOSS.jl","title":"Active Learning Problem","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"The BOSS.jl package currently only supports optimization problems out-of-the-box. However, BOSS.jl can be adapted for active learning easily by defining a suitable acquisition function (such as information gain or Kullback-Leibler divergence) to use instead of the expected improvement (see AcquisitionFunction). An acquisition function for active learning will usually not require the fitness function to be defined, so the fitness function can be omitted during problem definition (see BossProblem).","category":"page"},{"location":"#References","page":"BOSS.jl","title":"References","text":"","category":"section"},{"location":"","page":"BOSS.jl","title":"BOSS.jl","text":"[1] Bobak Shahriari et al. “Taking the human out of the loop: A review of Bayesian optimization”. In: Proceedings of the IEEE 104.1 (2015), pp. 148–175","category":"page"},{"location":"example/#Example","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"An illustrative problem is solved using BOSS.jl here. The source code is available at github.","category":"page"},{"location":"example/#The-Problem","page":"Example","title":"The Problem","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We have an expensive-to-evaluate blackbox function blackbox(x) -> (y, z).","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"function blackbox(x; noise_std=0.1)\n    y = exp(x[1]/10) * cos(2*x[1])\n    z = (1/2)^6 * (x[1]^2 - (15.)^2)\n    \n    y += rand(Normal(0., noise_std))\n    z += rand(Normal(0., noise_std))\n\n    return [y,z]\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We wish to maximize y such that x ∈ <0, 20> (constraint on input) and z < 0 (constraint on output).","category":"page"},{"location":"example/#Problem-Definition","page":"Example","title":"Problem Definition","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"First, we define the problem as an instance of BossProblem.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"problem() = BossProblem(;\n    fitness = LinFitness([1, 0]),   # maximize y\n    f = blackbox,\n    domain = Domain(;\n        bounds = ([0.], [20.]),     # x ∈ <0, 20>\n    ),\n    y_max = [Inf, 0.],              # z < 0\n    model = nonparametric(), # or `parametric()` or `semiparametric()`\n    data = init_data(),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use Fitness to define the objective. Here, the LinFitness([1, 0]) specifies that we wish to maximize 1*y + 0*z. (See also NonlinFitness.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the keyword f to provide the blackbox objective function.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the Domain structure to define the constraints on inputs.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We use the keyword y_max to define the constraints on outputs.","category":"page"},{"location":"example/#Surrogate-Model-Definition","page":"Example","title":"Surrogate Model Definition","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Now we define the surrogate model used to approximate the objective function based on the available data from previous evaluations.","category":"page"},{"location":"example/#Gaussian-Process","page":"Example","title":"Gaussian Process","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Usually, we will use a Gaussian process.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"nonparametric() = GaussianProcess(;\n    kernel = BOSS.Matern32Kernel(),\n    amplitude_priors = amplitude_priors(),\n    lengthscale_priors = lengthscale_priors(),\n    noise_std_priors = noise_std_priors(),\n)","category":"page"},{"location":"example/#Parametric-Model","page":"Example","title":"Parametric Model","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"If we have some knowledge about the blackbox function, we can define a parametric model.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"parametric() = NonlinearModel(;\n    predict = (x, θ) -> [\n        θ[1] * x[1] * cos(θ[2] * x[1]) + θ[3],\n        0.,\n    ],\n    theta_priors = fill(Normal(0., 1.), 3),\n    noise_std_priors = noise_std_priors(),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The function predict(x, θ) -> y defines our parametric model where θ are the model parameters which will be fitted based on the data.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The keyword theta_priors is used to define priors on the model parameters θ. The priors can be used to include our expert knowledge, to regularize the model, or a uniform prior can be used to not bias the model fit.","category":"page"},{"location":"example/#Semiparametric-Model","page":"Example","title":"Semiparametric Model","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can use the parametric model together with a Gaussian process to define the semiparametric model.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"semiparametric() = Semiparametric(\n    parametric(),\n    nonparametric(),\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"This allows us to leverage our expert knowledge incorporated in the parametric model while benefiting from the flexibility of the Gaussian process.","category":"page"},{"location":"example/#Hyperparameter-Priors","page":"Example","title":"Hyperparameter Priors","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We need to define all hyperparameters. Instead of defining scalar values, we will define priors over them and let BOSS fit their values based on the data. This alleviates the importance of our choice and allows for Bayesian inference if we wish to use it.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(If one wants to define some hyperparameters as scalars instead, a Dirac prior can be used and the hyperparameters will be skipped from model fitting.)","category":"page"},{"location":"example/#Evaluation-Noise","page":"Example","title":"Evaluation Noise","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"BOSS assumes Gaussian evaluation noise on the objective blackbox function. Noise std priors define our belief about the standard deviation of the noise of each individual output dimension.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"noise_std_priors() = fill(truncated(Normal(0., 0.1); lower=0.), 2)\n# noise_std_priors() = fill(Dirac(0.1), 2)","category":"page"},{"location":"example/#Amplitude","page":"Example","title":"Amplitude","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The amplitude of the Gaussian process expresses the expected deviation of the output values. We again define an amplitude prior for each individual output dimension.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"amplitude_priors() = fill(truncated(Normal(0., 5.); lower=0.), 2)\n# amplitude_priors() = fill(Dirac(5.), 2)","category":"page"},{"location":"example/#Length-Scales","page":"Example","title":"Length Scales","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Informally, the length scales of the Gaussian process define how far within the input domain does the model extrapolate the information obtained from the dataset. For each output dimension, we define a multivariate prior over all input dimensions. (In our case two 1-variate priors.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"lengthscale_priors() = fill(Product([truncated(Normal(0., 20/3); lower=0.)]), 2)\n# lengthscale_priors() = fill(Product(fill(Dirac(1.), 1)), 2)","category":"page"},{"location":"example/#Model-Fitter","page":"Example","title":"Model Fitter","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can specify the algorithm used to fit the model hyperparameters using the ModelFitter type.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We can fit the hyperparameters in a MAP fashion using the OptimizationMAP model fitter together with any algorithm from Optimization.jl and its extensions.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using OptimizationPRIMA\n\nmap_fitter() = OptimizationMAP(;\n    algorithm = NEWUOA(),\n    multistart = 20,\n    parallel = true,\n    rhoend = 1e-4,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Or we can use Bayesian inference and sample the parameters from their posterior (given by the priors and the data likelihood) using the TuringBI model fitter.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using Turing\n\nbi_fitter() = TuringBI(;\n    sampler = PG(20),\n    warmup = 100,\n    samples_in_chain = 10,\n    chain_count = 8,\n    leap_size = 5,\n    parallel = true,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"See also SamplingMAP and RandomFitter for more trivial model fitters suitable for simple experimentation with the package.","category":"page"},{"location":"example/#Acquisition-Maximizer","page":"Example","title":"Acquisition Maximizer","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can specify the algorithm used to maximize the acquisition function (in order to select the next evaluation point) by using the AcquisitionMaximizer type.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We can use the OptimizationAM maximizer together with any algorithm from Optimization.jl.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"acq_maximizer() = OptimizationAM(;\n    algorithm = BOBYQA(),\n    multistart = 20,\n    parallel = true,\n    rhoend = 1e-4,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Make sure to use an algorithm suitable for the given domain. (For example, in our case the domain is bounded by box constraints only, so we the BOBYQA optimization algorithm designed for box constraints problems.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"See also GridAM, RandomAM for more trivial acquisition maximizers suitable for simple experimentation with the package.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The SequentialBatchAM can be used to wrap any of the other acquisition maximizers to enable objective function evaluation in batches.","category":"page"},{"location":"example/#Acquisition-Function","page":"Example","title":"Acquisition Function","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"The acquisition function defines how the next evaluation point is selected in each iteration. The acquisition function is maximized by the acquisition maximizer algorithm (discussed in the previous section).","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Currently, the only implemented acquisition function is the ExpectedImprovement acquisition most commonly used in Bayesian optimization.","category":"page"},{"location":"example/#Miscellaneous","page":"Example","title":"Miscellaneous","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Finally, we can define the termination condition using the TermCond type. Currently, the only available termination condition is the trivial IterLimit condition. (However, one can simply define his own termination condition by extending the TermCond type.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The BossOptions structure can be used to change other miscellaneous settings.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The PlotCallback can be provided in BossOptions to enable plotting of the BO procedure. This can be useful for initial experimentation with the package. Note that the plotting only works for 1-dimensional input domains.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using Plots\n\noptions() = BossOptions(;\n    info = true,\n    callback = PlotCallback(Plots;\n        f_true = x->blackbox(x; noise_std=0.),\n    ),\n)","category":"page"},{"location":"example/#Run-BOSS","page":"Example","title":"Run BOSS","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Once we define the problem and all hyperparameters, we can run the BO procedure by calling the bo! function.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"bo!(problem();\n    model_fitter = map_fitter(), # or `bi_fitter()`\n    acq_maximizer = acq_maximizer(),\n    acquisition = ExpectedImprovement(),\n    term_cond = IterLimit(10),\n    options = options(),\n)","category":"page"}]
}
