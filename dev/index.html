<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>BOSS.jl · BOSS.jl</title><meta name="title" content="BOSS.jl · BOSS.jl"/><meta property="og:title" content="BOSS.jl · BOSS.jl"/><meta property="twitter:title" content="BOSS.jl · BOSS.jl"/><meta name="description" content="Documentation for BOSS.jl."/><meta property="og:description" content="Documentation for BOSS.jl."/><meta property="twitter:description" content="Documentation for BOSS.jl."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>BOSS.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>BOSS.jl</a><ul class="internal"><li><a class="tocitem" href="#Bayesian-Optimization"><span>Bayesian Optimization</span></a></li><li><a class="tocitem" href="#Optimization-Problem"><span>Optimization Problem</span></a></li><li><a class="tocitem" href="#Active-Learning-Problem"><span>Active Learning Problem</span></a></li><li><a class="tocitem" href="#Surrogate-Model"><span>Surrogate Model</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="functions/">Functions</a></li><li><a class="tocitem" href="types/">Data Types &amp; Structures</a></li><li><a class="tocitem" href="example/">Example</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>BOSS.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>BOSS.jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/soldasim/BOSS.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/soldasim/BOSS.jl/blob/master/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="BOSS.jl"><a class="docs-heading-anchor" href="#BOSS.jl">BOSS.jl</a><a id="BOSS.jl-1"></a><a class="docs-heading-anchor-permalink" href="#BOSS.jl" title="Permalink"></a></h1><p>BOSS stands for &quot;Bayesian Optimization with Semiparametric Surrogate&quot;. BOSS.jl is a Julia package for Bayesian optimization. It provides a compact way to define an optimization problem and a surrogate model, and solve the problem. It allows changing the algorithms used for the subtasks of fitting the surrogate model and optimizing the acquisition function. Simple interfaces are defined for the use of custom surrogate models and/or algorithms for the subtasks.</p><h2 id="Bayesian-Optimization"><a class="docs-heading-anchor" href="#Bayesian-Optimization">Bayesian Optimization</a><a id="Bayesian-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Optimization" title="Permalink"></a></h2><p>Bayesian optimization is an abstract algorithm for blackbox optimization (or active learning) with expensive-to-evaluate objective functions. The general procedure is showcased by the pseudocode below. [1]</p><table><tr><th style="text-align: right"></th><th style="text-align: right"></th><th style="text-align: right"></th><th style="text-align: right"></th><th style="text-align: right"></th></tr><tr><td style="text-align: right"></td><td style="text-align: right"></td><td style="text-align: right"><img src="img/bo.png" alt="Bayesian optimization"/></td><td style="text-align: right"></td><td style="text-align: right"></td></tr><tr><td style="text-align: right"></td><td style="text-align: right"></td><td style="text-align: right"></td><td style="text-align: right"></td><td style="text-align: right"></td></tr></table><p>The real-valued acquisition function <span>$\alpha(x)$</span> defines the utility of a potential point for the next evaluation. Different acquisition functions are suitable for optimization and active learning problems. We maximize the acquisition function to select the most useful point for the next evaluation.</p><p>Once the optimum <span>$x_{n+1}$</span> of the acquisition function is obtained, we evaluate the blackbox objective function <span>$y_{n+1} \gets f(x_{n+1})$</span> and we augment the dataset.</p><p>Finally, we update the surrogate model according to the augmented dataset.</p><p>See [1] for more information on Bayesian optimization.</p><h2 id="Optimization-Problem"><a class="docs-heading-anchor" href="#Optimization-Problem">Optimization Problem</a><a id="Optimization-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-Problem" title="Permalink"></a></h2><p>The problem is defined using the <a href="types/#BOSS.BossProblem"><code>BossProblem</code></a> structure and it follows the formalization below.</p><p>We have some (noisy) blackbox objective function</p><p class="math-container">\[y = f(x) = f_t(x) + \epsilon \;,\]</p><p>where <span>$\epsilon \sim \mathcal{N}(0, \sigma_f^2)$</span> is a Gaussian noise. We are able to evaluate <span>$f(x_i)$</span> and obtain a noisy realization</p><p class="math-container">\[y_i \sim \mathcal{N}(f_t(x_i), \sigma_f^2) \;.\]</p><p>Our goal is to solve the following optimization problem</p><p class="math-container">\[\begin{aligned}
\text{max} \; &amp; \text{fit}(y) \\
\text{s.t.} \; &amp; y &lt; y_\text{max} \\
&amp; x \in \text{Domain} \;,
\end{aligned}\]</p><p>where <span>$\text{fit}(y)$</span> is a real-valued fitness function defined on the outputs, <span>$y_\text{max}$</span> is a vector defining constraints on outputs, and <span>$\text{Domain}$</span> defines constraints on inputs.</p><h2 id="Active-Learning-Problem"><a class="docs-heading-anchor" href="#Active-Learning-Problem">Active Learning Problem</a><a id="Active-Learning-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Active-Learning-Problem" title="Permalink"></a></h2><p>The BOSS.jl package currently only supports optimization problems out-of-the-box. However, BOSS.jl can be adapted for active learning easily by defining a suitable acquisition function (such as information gain or Kullback-Leibler divergence) to use instead of the expected improvement (see <a href="types/#BOSS.AcquisitionFunction"><code>AcquisitionFunction</code></a>). An acquisition function for active learning will usually not require the fitness function to be defined, so the fitness function can be omitted during problem definition (see <a href="types/#BOSS.BossProblem"><code>BossProblem</code></a>).</p><h2 id="Surrogate-Model"><a class="docs-heading-anchor" href="#Surrogate-Model">Surrogate Model</a><a id="Surrogate-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Surrogate-Model" title="Permalink"></a></h2><p>The surrogate model approximates the objective function based on the available data. It is defined using the <a href="types/#BOSS.SurrogateModel"><code>SurrogateModel</code></a> type. The BOSS.jl package provides a <a href="types/#BOSS.Parametric"><code>Parametric</code></a> model, a <a href="types/#BOSS.Nonparametric"><code>Nonparametric</code></a> model, and a <a href="types/#BOSS.Semiparametric"><code>Semiparametric</code></a> model combining the previous two.</p><p>The predictive distribution of the <a href="types/#BOSS.Parametric"><code>Parametric</code></a> model</p><p class="math-container">\[y \sim \mathcal{N}(m(x; \hat\theta), \hat\sigma_f^2)\]</p><p>is given by the parametric function <span>$m(x; \theta)$</span>, the parameter vector <span>$\hat\theta$</span>, and the estimated evaluation noise deviations <span>$\hat\sigma_f$</span>. The model is defined by the parametric function <span>$m(x; \theta)$</span> together with parameter priors <span>$\theta_i \sim p(\theta_i)$</span>. The parameters <span>$\hat\theta$</span> and the noise deviations <span>$\hat\sigma_f$</span> are estimated based on the current dataset.</p><p>The <a href="types/#BOSS.Nonparametric"><code>Nonparametric</code></a> model is just an alias for the <a href="types/#BOSS.GaussianProcess"><code>GaussianProcess</code></a> model. Gaussian process (GP) is a nonparametric model, so its predictive distribution is based on the whole dataset instead of some vector of parameters. The predictive distribution is given by equations 29, 30 in [1]. The model is defined by defining priors over all its hyperparameters (length scales, amplitudes).</p><p>The <a href="types/#BOSS.Semiparametric"><code>Semiparametric</code></a> model combines the previous two models. It is a Gaussian process, but uses the parametric model as the prior mean of the GP (the <span>$\mu_0(x)$</span> function in [1]). An alternative way of interpreting the semiparametric model is that it fits the data using a parametric model and uses a Gaussian process to model the residual errors of the parametric model. The model is defined by defining both the <a href="types/#BOSS.Parametric"><code>Parametric</code></a> and <a href="types/#BOSS.Nonparametric"><code>Nonparametric</code></a> models.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>[1] Bobak Shahriari et al. “Taking the human out of the loop: A review of Bayesian optimization”. In: Proceedings of the IEEE 104.1 (2015), pp. 148–175</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="functions/">Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Tuesday 15 October 2024 11:33">Tuesday 15 October 2024</span>. Using Julia version 1.11.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
